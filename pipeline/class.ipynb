{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename=\"/Users/pongk/project_data_pipeline/file/cleanse_bd.log\",\n",
    "                    format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "                    filemode='w',\n",
    "                    level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"/postgres-jdbc/postgresql-42.7.3.jar\") \\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataframe():\n",
    "    def __init__(self,table_name):\n",
    "        self.table_name = table_name\n",
    "        \"\"\"self.table =spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", \"jdbc:postgresql://localhost:5432/data_pipeline\") \\\n",
    "                    .option(\"dbtable\", self.table_name) \\\n",
    "                    .option(\"user\", \"postgres\") \\\n",
    "                    .option(\"password\", \"pong1234\") \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .load()\"\"\"\n",
    "    def get_table_name(self):\n",
    "        return self.table_name\n",
    "    def show(self):\n",
    "        return self.table.show()\n",
    "    def query_toshow(self,order):\n",
    "        self.table.createOrReplaceTempView(self.table_name)\n",
    "        spark.sql(order).show()\n",
    "        return spark.sql(order)\n",
    "    def query_andsave(self,order):\n",
    "        self.table.createOrReplaceTempView(self.table_name)\n",
    "        self.table = spark.sql(order)\n",
    "        \"\"\"spark.sql(order).show()\"\"\"\n",
    "        return self.table.count()\n",
    "    def null_check(self):\n",
    "        null_counts_per_column = []\n",
    "        for col_name in self.table.columns:\n",
    "            null_count = self.table.where(col(col_name).isNull()).count()\n",
    "\n",
    "            null_counts_per_column.append((col_name, null_count))\n",
    "            \n",
    "        for col_name, null_count in null_counts_per_column:\n",
    "            print(f\"Total number of null values in column '{col_name}': {null_count}\")\n",
    "    def describe(self):\n",
    "        self.table.describe().show()\n",
    "        return self.table.describe()\n",
    "    def count_len(self):\n",
    "        return self.table.count()\n",
    "    def info(self):\n",
    "        self.table.toPandas().info()\n",
    "    def isnull(self):\n",
    "        pan=self.table.toPandas()\n",
    "        pan= pan[pan.isnull().any(axis=1)]\n",
    "        spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        if pan.empty:\n",
    "            return \"0\"\n",
    "        else:\n",
    "            info_ = spark.createDataFrame(pan)\n",
    "            info_.show()\n",
    "            return info_\n",
    "\n",
    "\n",
    "    def dropna(self):\n",
    "        self.table.dropna().show()\n",
    "        return self.table.dropna()\n",
    "    def topandas(self):\n",
    "        return self.table.toPandas()\n",
    "    def query_topandas(self,order):\n",
    "        self.table.createOrReplaceTempView(self.table_name)\n",
    "        return spark.sql(order).toPandas()\n",
    "    def save_database(self,location,name,user,password):\n",
    "        self.table.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:postgresql:\"+location) \\\n",
    "            .option(\"dbtable\",name) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "    def save_toparquet(self,locate_name):\n",
    "        spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccesfuljobs\",\"false\")\n",
    "        self.table = self.table.repartition(1)\n",
    "        self.table.coalesce(1).write\\\n",
    "            .format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f'{locate_name}')\n",
    "    def dropDuplicates(self):\n",
    "        self.table = self.table.dropDuplicates()\n",
    "        return self.table\n",
    "    def drop_column(self,col):\n",
    "        self.table= self.table.drop(*col)\n",
    "    def merge(self,df2,column_1,column_2,HOW):\n",
    "        merged_df = self.table.join(df2.table,self.table[column_1]==df2.table[column_2], how=HOW)\n",
    "        merged_df = merged_df.orderBy(\"uuid\")\n",
    "        self.table = merged_df\n",
    "        return merged_df\n",
    "    def Schema(self):\n",
    "        self.table.printSchema()\n",
    "    def col(self,col_name):\n",
    "        self.table.where(col(col_name))\n",
    "    def change_dtype(self,col1,type):\n",
    "        self.table = self.table.withColumn(col1, self.table[col1].cast(type))\n",
    "    def orderby(self,column):\n",
    "        self.table=self.table.orderBy(column)\n",
    "    def save_tocsv(self,locate):\n",
    "        spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccesfuljobs\",\"false\")\n",
    "        self.table = self.table.repartition(1)\n",
    "        self.table.write.csv(locate, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataframe_connect(dataframe):\n",
    "    def __init__(self,table_name,database):\n",
    "        super().__init__(table_name)\n",
    "        self.table =spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", f\"jdbc:postgresql://localhost:5432/{database}\") \\\n",
    "                    .option(\"dbtable\", self.table_name) \\\n",
    "                    .option(\"user\", \"postgres\") \\\n",
    "                    .option(\"password\", \"pong1234\") \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataframe_pandas(dataframe):\n",
    "    def __init__(self,pandas,name):\n",
    "        super().__init__(name)\n",
    "        spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        self.table = spark.createDataFrame(pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_info(df):\n",
    "    print(\"----------schema----------\")\n",
    "    df.Schema()\n",
    "    print(\"number of row :\"+str(df.count_len()))\n",
    "    print(\"----------info----------\")\n",
    "    df.info()\n",
    "    print(\"----------null check----------\")\n",
    "    df.null_check()\n",
    "    if df.isnull() == \"0\":\n",
    "        print(\"null count:0\")\n",
    "    else:\n",
    "        print(\"null count:\"+str(df.isnull().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_student_table_to_pandas(df):\n",
    "    import matplotlib as mpl\n",
    "    mpl.rcParams['figure.facecolor'] = 'white'\n",
    "    import matplotlib.pyplot as plt\n",
    "    df.query_andsave(f\"SELECT uuid,name,dob,sex,contact_info,job_id,num_course_taken,current_career_path_id,time_spent_hrs,DATEDIFF(year, dob, CURRENT_DATE) AS age, FLOOR(age / 10) * 10 AS age_group FROM {df.get_table_name()}\")\n",
    "    df = df.topandas()\n",
    "    df[\"contact_info\"] =df[\"contact_info\"].apply(lambda x:ast.literal_eval(x))\n",
    "    explode_contact = pd.json_normalize(df[\"contact_info\"])\n",
    "    df = pd.concat([df.drop('contact_info',axis=1),explode_contact],axis=1)\n",
    "    mail = df['mailing_address'].str.split(',',expand = True)\n",
    "    df['street'] = None\n",
    "    df['city'] = None\n",
    "    df['state'] = None\n",
    "    df['zip_code'] = None\n",
    "    df['street'] = mail[0]\n",
    "    df['city'] = mail[1]\n",
    "    df['state']= mail[2]\n",
    "    df['zip_code']= mail[3]\n",
    "    df = df.drop('mailing_address',axis=1)\n",
    "    df['current_career_path_id'] = pd.to_numeric(df['current_career_path_id'], errors='coerce')\n",
    "    df['time_spent_hrs'] = pd.to_numeric(df['time_spent_hrs'], errors='coerce')\n",
    "    df['job_id'] = pd.to_numeric(df['job_id'], errors='coerce')\n",
    "    miss_data =  df[df[['num_course_taken']].isnull().any(axis=1)]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 7))\n",
    "\n",
    "    sg = (df.groupby('sex').count()['uuid']/len(df)).rename('complete')\n",
    "    mg = (miss_data.groupby('sex').count()['uuid']/len(miss_data)).rename('incomplete')\n",
    "    vs_pandas_1 = pd.concat([sg,mg],axis=1)\n",
    "    print(vs_pandas_1)\n",
    "    vs_pandas_1.plot.bar(ax=axs[0])\n",
    "\n",
    "    sg = (df.groupby('job_id').count()['uuid']/len(df)).rename('complete')\n",
    "    mg = (miss_data.groupby('job_id').count()['uuid']/len(miss_data)).rename('incomplete')\n",
    "    vs_pandas_2 = pd.concat([sg,mg],axis=1)\n",
    "    print(vs_pandas_2)\n",
    "    vs_pandas_2.plot.bar(ax=axs[1])\n",
    "\n",
    "    sg = (df.groupby('age_group').count()['uuid']/len(df)).rename('complete')\n",
    "    mg = (miss_data.groupby('age_group').count()['uuid']/len(miss_data)).rename('incomplete')\n",
    "    vs_pandas_3 = pd.concat([sg,mg],axis=1)\n",
    "    print(vs_pandas_3)\n",
    "    vs_pandas_3.plot.bar(ax=axs[2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df = df.dropna(subset=['num_course_taken'])\n",
    "    miss_data_job_id = df[df[['job_id']].isnull().any(axis=1)]\n",
    "    miss_data = pd.concat([miss_data,miss_data_job_id])\n",
    "    df = df.dropna(subset=['job_id'])\n",
    "    \n",
    "\n",
    "    df['current_career_path_id'] = np.where(df['current_career_path_id'].isnull(),0,df['current_career_path_id'])\n",
    "    df['time_spent_hrs'] = np.where(df['time_spent_hrs'].isnull(),0,df['time_spent_hrs'])\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.sort_values(by='uuid', ascending=True)\n",
    "    miss_data = miss_data.drop_duplicates()\n",
    "    miss_data = miss_data.sort_values(by='uuid', ascending=True)\n",
    "\n",
    "    return (df,miss_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(_df_student,_df_path,_df_job):\n",
    "    try:\n",
    "        _df_student = _df_student.rename(columns={\"job_id\": \"JOB_ID_1\"})\n",
    "        if _df_student.empty:\n",
    "            raise ValueError(\"Input dataframe clean_new_students is empty.\")\n",
    "        if _df_path.empty:\n",
    "            raise ValueError(\"Input dataframe clean_career_path is empty.\")\n",
    "        if _df_job.empty:\n",
    "            raise ValueError(\"Input dataframe clean_student_jobs is empty.\")\n",
    "        df_student=dataframe_pandas(_df_student,'df1')\n",
    "        df_career_path = dataframe_pandas(_df_path,'df2')\n",
    "        df_student_jobs = dataframe_pandas(_df_job,'df3')\n",
    "        df_student.merge(df_career_path,'current_career_path_id','career_path_id','left')\n",
    "        df_student.merge(df_student_jobs,'JOB_ID_1','job_id','left')\n",
    "        try:\n",
    "            df_student.drop_column([\"JOB_ID_1\",'career_path_id'])\n",
    "        except KeyError as e:\n",
    "            print(f\"Column drop error: {e}\")    \n",
    "    except Exception as e:  # Catch general exceptions (broader handling)\n",
    "        print(f\"Unexpected error: {e}\")   \n",
    "    else: \n",
    "        return df_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_career_path_topandas(df):\n",
    "    df = df.topandas()\n",
    "    not_applicable = {'career_path_id':0,'career_path_name':'not applicable','hours_to_complete':0}\n",
    "    df.loc[len(df)] = not_applicable\n",
    "    df =df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_job_to_pandas(df):\n",
    "    df=df.topandas()\n",
    "    df =df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nulls(df):\n",
    "    df_missing = df[df.isnull().any(axis=1)]\n",
    "    cnt_missing = len(df_missing)\n",
    "    try:\n",
    "        assert cnt_missing == 0 , \"there are \" + str(cnt_missing)+ \"null in the table\"\n",
    "    except AssertionError as ae:\n",
    "        logger.exception(ae)\n",
    "    else:\n",
    "        print(\"No null row found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_schema(local_df,db_df):\n",
    "    errors =0\n",
    "    mismatch_columns = []\n",
    "    for col in db_df:\n",
    "        try:\n",
    "            if local_df[col].dtype != db_df[col].dtypes:\n",
    "                errors +=1\n",
    "                mismatch_columns.append(col)\n",
    "                logger.error(f\"Data type mismatch for column '{col}': Local dtype - {local_df[col].dtype}, DB dtype - {db_df[col].dtype}\")\n",
    "        except NameError as ne:\n",
    "            logger.exception(ne)\n",
    "            raise ne \n",
    "    errors = 1\n",
    "    if errors > 0 :\n",
    "        assert_err_msg = str(errors) + \" column(s) dtype aren't the same\"\n",
    "        logger.exception(assert_err_msg)\n",
    "    assert errors == 0, assert_err_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_num_cols(local_df,db_df):\n",
    "    try:\n",
    "        assert len(local_df.columns) == len(db_df.columns)\n",
    "    except AssertionError as ae:\n",
    "        logger.exception(ae)\n",
    "        raise ae\n",
    "    else:\n",
    "        print(\"Number of columns are same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_for_path_id(student,career_paths):\n",
    "    studebt_table = student.current_career_path_id.unique()\n",
    "    is_subset = np.isin(studebt_table,career_paths.career_path_id.unique())\n",
    "    missing_id = studebt_table[~is_subset]\n",
    "    try:\n",
    "        assert len(missing_id) == 0 , \"Missing career_path_id(s)\"+str(list(missing_id))+\"in 'career_paths' table\" \n",
    "    except AssertionError as ae:\n",
    "        logger.exception(ae)\n",
    "        raise ae\n",
    "    else:\n",
    "        print(\"all career_path_ids are present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_for_job_id(student,student_jobs):\n",
    "    student_table = student.job_id.unique()\n",
    "    is_subset = np.isin(student_table, student_jobs.job_id.unique())\n",
    "    missing_id = student_table[~is_subset]\n",
    "    try:\n",
    "        assert len(missing_id) == 0, \"Missing job_id(s):\" +str(list(missing_id))+\" in 'student_jobs' table\"\n",
    "    except AssertionError as ae:\n",
    "        logger.exception (ae)\n",
    "        raise ae\n",
    "    else:\n",
    "        print(\"All job_ids are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(locate_update,locate_clean):\n",
    "    logger.info(\"Start Log\")\n",
    "    with open('C:\\\\Users\\\\pongk\\\\project_data_pipeline\\\\file\\\\changelog.md','r') as f:\n",
    "        lines = f.readlines()\n",
    "    if len(lines) == 0:\n",
    "        next_ver = 0\n",
    "    else:\n",
    "        next_ver = int(lines[0].replace(\"## 0.0.\", \"\"))+1\n",
    "    table_student = dataframe_connect(\"cademycode_students\",locate_update)\n",
    "    table_career_path = dataframe_connect(\"cademycode_courses\",locate_update)\n",
    "    table_student_jobs = dataframe_connect(\"cademycode_student_jobs\",locate_update)\n",
    "    pd_student = table_student.topandas()\n",
    "    \n",
    "    try:\n",
    "        clean_db = dataframe_connect(\"cleanse_data\",locate_clean)\n",
    "        pd_clean_db = clean_db.topandas()\n",
    "        missing_db = dataframe_connect(\"missing_data\",locate_clean)\n",
    "        pd_missing = missing_db.topandas()\n",
    "        new_student = pd_student[~np.isin(pd_student.uuid.unique(),pd_clean_db.uuid.unique())]\n",
    "    except:\n",
    "        new_student = pd_student\n",
    "        clean_db =[]\n",
    "        \n",
    "    print(new_student)\n",
    "    clean_new_students, missing_data = cleanse_student_table_to_pandas(dataframe_pandas(new_student,'new_student'))\n",
    "    \n",
    "    try:\n",
    "        new_missing_data = missing_data[~np.isin(missing_data.uuid.unique(),pd_missing.uuid.unique())]\n",
    "    except:\n",
    "        new_missing_data = missing_data\n",
    "\n",
    "\n",
    "    if len(new_missing_data)>0:\n",
    "        incomplete = dataframe_pandas(new_missing_data,'incomplete')\n",
    "        incomplete.save_database(locate_update,\"missing_data\",\"postgres\",\"pong1234\")\n",
    "        incomplete.save_toparquet(\"/Users/pongk/project_data_pipeline/file/miss_data.parquet\")\n",
    "        incomplete.save_tocsv(\"/Users/pongk/project_data_pipeline/file/miss_data.csv\")\n",
    "    if len(clean_new_students)>0:\n",
    "        clean_career_path = cleanse_career_path_topandas(table_career_path)\n",
    "        clean_student_jobs = cleanse_job_to_pandas(table_student_jobs)\n",
    "        test_for_job_id(clean_new_students,clean_student_jobs)\n",
    "        test_for_path_id(clean_new_students,clean_career_path)\n",
    "        df_clean = merge(clean_new_students,clean_career_path,clean_student_jobs)\n",
    "        if len(clean_db)>0:\n",
    "            test_num_cols(df_clean.topandas(),pd_clean_db)\n",
    "            test_schema(df_clean.topandas(),pd_clean_db)\n",
    "        \n",
    "        test_nulls(df_clean.topandas())\n",
    "\n",
    "        df_clean.save_database(locate_update,\"cleanse_data\",\"postgres\",\"pong1234\")\n",
    "        clean_db = dataframe_connect(\"cleanse_data\",locate_update)\n",
    "        clean_db.save_toparquet(\"/Users/pongk/project_data_pipeline/file/final_df.parquet\")\n",
    "        clean_db.save_tocsv(\"/Users/pongk/project_data_pipeline/file/final_df.csv\")\n",
    "        new_lines =[\n",
    "            '## 0.0.'+str(next_ver)+'\\n'+\n",
    "            '### Added\\n' +\n",
    "            '_ '+ str(len(df_clean.topandas()))+'more data to database of raw data \\n'+\n",
    "            '_ '+ str(len(new_missing_data))+ 'new missing data to incomplete_data table\\n'+\n",
    "            '\\n'\n",
    "        ]\n",
    "        w_lines =''.join(new_lines + lines)\n",
    "        with open('/Users/pongk/project_data_pipeline/file/changelog.md','w') as f:\n",
    "            for line in w_lines:\n",
    "                f.write(line)\n",
    "    else:\n",
    "        print('no new data')\n",
    "        logger.info('no new data')\n",
    "    logger.info(\"END Log\")\n",
    "    spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"data_pipe_line_update\",\"data_pipeline\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
